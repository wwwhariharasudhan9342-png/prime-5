!pip install -q transformers accelerate torch huggingface_hub

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

print("Paste the Para to summarize:\n")
long_text = input()

prompt = f"""
Summarize the following text into a short and clear paragraph:

Text:
{long_text}

Summary:
"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

output = model.generate(
    **inputs,
    max_new_tokens=120,
    temperature=0.7
)

summary = tokenizer.decode(output[0], skip_special_tokens=True)

print("\n--- AI Generated Summary ---\n")
print(summary.split("Summary:")[-1].strip())